{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLFrozenLakeMedium",
      "provenance": [],
      "authorship_tag": "ABX9TyORBtfWxkh49Ph+dvITV0R6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oektomo/KelasRLG2/blob/master/05SarsaLFrozenLakeMedium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i545SMvqiI-W",
        "outputId": "a0401868-45ff-4adb-e4cb-eacf1c8cc8bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time:  7.8869\n",
            "[[6.67117711e-04 6.08296411e-04 3.72464171e-03 4.91911110e-04]\n",
            " [2.89877090e-04 3.71758958e-04 4.44159610e-04 7.59382476e-04]\n",
            " [3.46213678e-04 4.87911502e-05 1.51722156e-04 1.07606541e-03]\n",
            " [2.29390526e-04 1.34712078e-06 4.06446224e-05 1.41407842e-04]\n",
            " [8.69681713e-04 9.59552103e-04 1.06223076e-04 2.06502938e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.09253438e-03 4.73925096e-06 5.61018692e-04 7.49127871e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.10481341e-03 8.96263360e-02 1.89746039e-04 2.74254660e-03]\n",
            " [5.91125609e-01 1.23085051e-01 9.15987072e-04 2.95287462e-02]\n",
            " [4.86013919e-03 2.01257233e-01 1.65745595e-02 1.48268435e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.76742939e-02 1.04511062e-01 3.47393338e-01 1.15294171e-01]\n",
            " [1.84002724e-01 4.77350695e-01 4.68784488e-01 8.58719648e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "# https://medium.com/swlh/introduction-to-reinforcement-learning-coding-sarsa-part-4-2d64d6e37617\n",
        "# https://medium.com/swlh/introduction-to-reinforcement-learning-coding-q-learning-part-3-9778366a41c0\n",
        "import gym\n",
        "import numpy as np\n",
        "import time, pickle, os\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "epsilon = 0.9\n",
        "# min_epsilon = 0.1\n",
        "# max_epsilon = 1.0\n",
        "# decay_rate = 0.01\n",
        "\n",
        "total_episodes = 10000\n",
        "max_steps = 100\n",
        "\n",
        "lr_rate = 0.81\n",
        "gamma = 0.96\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    \n",
        "def choose_action(state):\n",
        "\taction=0\n",
        "\tif np.random.uniform(0, 1) < epsilon:\n",
        "\t\taction = env.action_space.sample()\n",
        "\telse:\n",
        "\t\taction = np.argmax(Q[state, :])\n",
        "\treturn action\n",
        "\n",
        "def learn(state, state2, reward, action, action2):\n",
        "\tpredict = Q[state, action]\n",
        "\ttarget = reward + gamma * Q[state2, action2]\n",
        "\tQ[state, action] = Q[state, action] + lr_rate * (target - predict)\n",
        "\n",
        "# Start\n",
        "rewards=0\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "\tt = 0\n",
        "\tstate = env.reset()\n",
        "\taction = choose_action(state)\n",
        "    \n",
        "\twhile t < max_steps:\n",
        "\t\t#env.render()\n",
        "\n",
        "\t\tstate2, reward, done, info = env.step(action)\n",
        "\n",
        "\t\taction2 = choose_action(state2)\n",
        "\n",
        "\t\tlearn(state, state2, reward, action, action2)\n",
        "\n",
        "\t\tstate = state2\n",
        "\t\taction = action2\n",
        "\n",
        "\t\tt += 1\n",
        "\t\trewards+=1\n",
        "\n",
        "\t\tif done:\n",
        "\t\t\tbreak\n",
        "  # epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n",
        "  # os.system('clear')\n",
        "\t#\ttime.sleep(0.1)\n",
        "\n",
        "    \n",
        "print (\"Score over time: \", rewards/total_episodes)\n",
        "print(Q)\n",
        "\n",
        "#with open(\"frozenLake_qTable_sarsa.pkl\", 'wb') as f:\n",
        "#\tpickle.dump(Q, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y46J16TljJL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}