{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c4d190",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/oektomo/KelasRLG2/blob/master/DPFrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b10acc2",
   "metadata": {
    "id": "7b10acc2"
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f133b24",
   "metadata": {
    "id": "1f133b24"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba1ef9d",
   "metadata": {
    "id": "aba1ef9d"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=0.9, theta=1e-9, max_iterations=1e9):\n",
    "        # Number of evaluation iterations\n",
    "        evaluation_iterations = 1\n",
    "        # Initialize a value function for each state as zero\n",
    "        V = np.zeros(environment.nS)\n",
    "        # Repeat until change in value is below the threshold\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Initialize a change of value function as zero\n",
    "                delta = 0\n",
    "                # Iterate though each state\n",
    "                for state in range(environment.nS):\n",
    "                       # Initial a new value of current state\n",
    "                       v = 0\n",
    "                       # Try all possible actions which can be taken from this state\n",
    "                       for action, action_probability in enumerate(policy[state]):\n",
    "                             # Check how good next state will be\n",
    "                             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                                  # Calculate the expected value\n",
    "                                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "                       # Calculate the absolute change of value function\n",
    "                       delta = max(delta, np.abs(V[state] - v))\n",
    "                       # Update value function\n",
    "                       V[state] = v\n",
    "                \n",
    "                evaluation_iterations += 1\n",
    "                \n",
    "                # Terminate if value change is insignificant\n",
    "                if delta < theta:\n",
    "                        print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "                        return V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea8822c",
   "metadata": {
    "id": "bea8822c"
   },
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "        action_values = np.zeros(environment.nA)\n",
    "        for action in range(environment.nA):\n",
    "                for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                        action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05fc58e5",
   "metadata": {
    "id": "05fc58e5"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=0.9, max_iterations=1e9):\n",
    "        # Start with a random policy\n",
    "        #num states x num actions / num actions\n",
    "        environment = environment.unwrapped\n",
    "        policy = np.ones([environment.nS, environment.nA]) / environment.nA\n",
    "        # Initialize counter of evaluated policies\n",
    "        evaluated_policies = 1\n",
    "        # Repeat until convergence or critical number of iterations reached\n",
    "        for i in range(int(max_iterations)):\n",
    "                stable_policy = True\n",
    "                # Evaluate current policy\n",
    "                V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "                # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "                for state in range(environment.nS):\n",
    "                        # Choose the best action in a current state under current policy\n",
    "                        current_action = np.argmax(policy[state])\n",
    "                        # Look one step ahead and evaluate if current action is optimal\n",
    "                        # We will try every possible action in a current state\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select a better action\n",
    "                        best_action = np.argmax(action_value)\n",
    "                        # If action didn't change\n",
    "                        if current_action != best_action:\n",
    "                                stable_policy = True\n",
    "                                # Greedy policy update\n",
    "                                policy[state] = np.eye(environment.nA)[best_action]\n",
    "                evaluated_policies += 1\n",
    "                # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "                if stable_policy:\n",
    "                        print(f'Evaluated {evaluated_policies} policies.')\n",
    "                        return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e83a876d",
   "metadata": {
    "id": "e83a876d"
   },
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=0.9, theta=1e-9, max_iterations=1e9):\n",
    "        # Initialize state-value function with zeros for each environment state\n",
    "        V = np.zeros(environment.nS)\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Early stopping condition\n",
    "                delta = 0\n",
    "                # Update each state\n",
    "                for state in range(environment.nS):\n",
    "                        # Do a one-step lookahead to calculate state-action values\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select best action to perform based on the highest state-action value\n",
    "                        best_action_value = np.max(action_value)\n",
    "                        # Calculate change in value\n",
    "                        delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "                        # Update the value function for current state\n",
    "                        V[state] = best_action_value\n",
    "                        # Check if we can stop\n",
    "                if delta < theta:\n",
    "                        print(f'Value-iteration converged at iteration#{i}.')\n",
    "                        break\n",
    "\n",
    "        # Create a deterministic policy using the optimal value function\n",
    "        policy = np.zeros([environment.nS, environment.nA])\n",
    "        for state in range(environment.nS):\n",
    "                # One step lookahead to find the best action for this state\n",
    "                action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                # Select best action based on the highest state-action value\n",
    "                best_action = np.argmax(action_value)\n",
    "                # Update the policy to perform a better action at a current state\n",
    "                policy[state, best_action] = 1.0\n",
    "        return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdeb3c4e",
   "metadata": {
    "id": "fdeb3c4e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "        wins = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(n_episodes):\n",
    "                terminated = False\n",
    "                state = environment.reset()\n",
    "                while not terminated:\n",
    "                        # Select best action to perform in a current state\n",
    "                        action = np.argmax(policy[state])\n",
    "                        # Perform an action an observe how environment acted in response\n",
    "                        next_state, reward, terminated, info = environment.step(action)\n",
    "                        # Summarize total reward\n",
    "                        \n",
    "                        # Update current state\n",
    "                        state = next_state\n",
    "                        # Calculate number of wins over episodes\n",
    "                        if terminated and reward == 1.0:\n",
    "                                wins += 1\n",
    "                                total_reward += reward\n",
    "        average_reward = total_reward / n_episodes\n",
    "        return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602c3de4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "602c3de4",
    "outputId": "6160ca35-c510-4061-ebae-0211fd57c1b0"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'nS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m environment \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Search for an optimal policy using policy iteration\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m policy, V \u001b[38;5;241m=\u001b[39m \u001b[43miteration_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Apply best policy to the real environment\u001b[39;00m\n\u001b[1;32m     12\u001b[0m wins, total_reward, average_reward \u001b[38;5;241m=\u001b[39m play_episodes(environment, n_episodes, policy)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[0;34m(environment, discount_factor, max_iterations)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy_iteration\u001b[39m(environment, discount_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e9\u001b[39m):\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;66;03m# Start with a random policy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m#num states x num actions / num actions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         environment \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39munwrapped\n\u001b[0;32m----> 5\u001b[0m         policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones([\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnS\u001b[49m, environment\u001b[38;5;241m.\u001b[39mnA]) \u001b[38;5;241m/\u001b[39m environment\u001b[38;5;241m.\u001b[39mnA\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Initialize counter of evaluated policies\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         evaluated_policies \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'nS'"
     ]
    }
   ],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "# Functions to find best policy\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "for iteration_name, iteration_func in solvers:\n",
    "        # Load a Frozen Lake environment\n",
    "        environment = gym.make('FrozenLake-v1')\n",
    "        # Search for an optimal policy using policy iteration\n",
    "        policy, V = iteration_func(environment.env)\n",
    "        # Apply best policy to the real environment\n",
    "        wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "        print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "        print(f'{iteration_name} :: total reward over {n_episodes} episodes = {total_reward}')\n",
    "        print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc1317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "DPFrozenLake.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
